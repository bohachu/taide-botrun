{"id":"CR-013","category":{"main":"跨領域閱讀","sub":"社會議題","tags":["人工智慧","演算法偏見","科技倫理"]},"source":{"type":"gsat_based","reference_year":"114","curriculum_topic":"科技與社會"},"difficulty":{"level":5,"discrimination":0.73},"question":{"stem":"閱讀下文，回答問題：\n\n人工智慧越來越多用於重要決策：銀行貸款審核、刑事司法風險評估、履歷篩選。AI被期待比人類更客觀，因為它「只看數據，不帶偏見」。但研究揭示，AI可能複製甚至放大人類偏見。\n\n問題出在訓練資料。如果歷史資料中，某族群較少獲得貸款（可能因過去歧視），AI學習這個模式後，會認為該族群「信用風險高」，繼續拒絕貸款——歧視被演算法固化。更隱蔽的是「代理變數」：AI不直接使用種族資料（因法律禁止），但郵遞區號、姓名等變數高度相關於種族，AI透過這些「代理」間接歧視。\n\n亞馬遜曾開發AI履歷篩選系統，發現它歧視女性應徵者，因為訓練資料多為男性工程師履歷，AI學到「男性特徵」（如「男子籃球隊隊長」）是好履歷的指標。這凸顯了「垃圾進，垃圾出」原則：有偏見的資料訓練出有偏見的AI。\n\n根據上文，AI系統產生偏見的主要原因為何？","context":null,"type":"single_choice","options":[{"label":"A","content":"AI的運算能力不足，無法處理複雜的社會變數"},{"label":"B","content":"訓練資料反映了歷史與社會的既有偏見"},{"label":"C","content":"AI缺乏人類的道德判斷能力，只能機械式計算"},{"label":"D","content":"程式設計師故意在演算法中加入歧視性規則"}]},"answer":{"correct":"B","acceptable_alternatives":[]},"reasoning":{"steps":[{"step_number":1,"action":"理解AI偏見的來源","explanation":"文中明確指出「問題出在訓練資料」，如果歷史資料中有偏見（如過去歧視導致某族群較少獲貸款），AI會學習並複製這個偏見。","key_point":"訓練資料是偏見的源頭"},{"step_number":2,"action":"理解「垃圾進，垃圾出」原則","explanation":"亞馬遜案例：訓練資料多為男性履歷→AI學到男性特徵是優勢→歧視女性。資料的偏見決定了AI的偏見。","key_point":"資料品質決定AI輸出"},{"step_number":3,"action":"理解代理變數問題","explanation":"即使不直接使用敏感資訊（種族），歷史與社會的不平等會反映在相關變數（郵遞區號、姓名）上，AI透過這些變數間接學到偏見。","key_point":"社會不平等嵌入在資料關聯中"},{"step_number":4,"action":"檢視選項 B","explanation":"「訓練資料反映了歷史與社會的既有偏見」精確描述AI偏見的根源：不是AI本身的問題，而是資料承載的社會現實。","key_point":"社會偏見→資料偏見→AI偏見"},{"step_number":5,"action":"確認答案","explanation":"選項 B 準確捕捉文章的核心論點和兩個案例的共同機制。","key_point":"選 B"}],"principle":"理解AI倫理議題需：(1)認識AI是從資料學習，而非憑空創造；(2)理解歷史偏見如何嵌入資料；(3)區分直接歧視與間接歧視（代理變數）；(4)掌握「客觀」≠「公平」的概念。","knowledge_points":["機器學習原理","演算法偏見","訓練資料","代理變數","科技倫理","社會不平等"]},"traps":[{"wrong_option":"A","trap_type":"technical_misattribution","why_wrong":"問題不在運算能力，而在資料品質。即使運算能力再強，有偏見的資料仍會產生有偏見的結果。","how_to_avoid":"區分「技術能力」與「資料品質」問題，AI偏見主要是後者。","common_mistake_rate":0.20},{"wrong_option":"C","trap_type":"oversimplification","why_wrong":"雖然AI缺乏道德判斷，但問題的根源不是「機械式計算」，而是「學習了有偏見的資料」。即使AI有道德判斷，若資料有偏見仍會學到偏見。","how_to_avoid":"抓住因果鏈的起點：資料偏見是根本原因，缺乏道德判斷只是讓它無法自我修正。","common_mistake_rate":0.25},{"wrong_option":"D","trap_type":"conspiracy_thinking","why_wrong":"文中沒有暗示故意加入歧視規則。相反，亞馬遜發現偏見後停用系統，說明是「無意中」產生的。問題在於資料的隱藏偏見，而非惡意設計。","how_to_avoid":"區分「系統性偏見」（結構性問題）與「個人惡意」（故意歧視），前者更隱蔽也更常見。","common_mistake_rate":0.15}],"learning":{"if_correct":"延伸學習：探討AI公平性的技術方法（如公平性指標、去偏見演算法）、監管政策（如歐盟AI法案），以及參與式設計的重要性。","if_wrong":{"feedback":"此題考查AI倫理的理解，需要：(1)認識機器學習的資料依賴性；(2)理解歷史偏見如何嵌入資料；(3)區分技術問題與社會問題；(4)避免陰謀論思維。","review_topics":["機器學習基礎","演算法公平性","科技倫理","社會不平等的再生產"],"related_questions":["CR-014","CR-011"]}},"verification":{"verified":true,"verification_source":"AI倫理研究、演算法公平性文獻","verification_date":"2025-12-13","confidence":0.95},"metadata":{"created_date":"2025-12-13","version":"1.0","author":"TAIDE-Botrun"}}